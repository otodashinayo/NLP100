{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"89.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1CI3fSV5WdLJdmytni6wEbJd3Kzr3VLiz","authorship_tag":"ABX9TyOL+/limKe29Jwf/x+vvRnJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"danohU0zOPCP"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD3jng2BG2LU"},"source":["from os import path\n","import json\n","import pandas as pd\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","from torch.optim import SGD\n","from torch.optim.lr_scheduler import ExponentialLR\n","from transformers import BertModel\n","import matplotlib.pyplot as plt\n","\n","base = \"/content/drive/MyDrive/NLP100/ch09\"\n","\n","fp_train = \"80/train.csv\"\n","fp_valid = \"80/valid.csv\"\n","fp_words = \"80/word_ids.json\"\n","df_train = pd.read_csv(path.join(base, fp_train), index_col=0)\n","df_valid = pd.read_csv(path.join(base, fp_valid), index_col=0)\n","word_ids = json.load(open(path.join(base, fp_words), \"r\"))\n","df_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sWzPtYPH1dE"},"source":["num_words_of_title = max([len(title.split()) for title in df_train[\"TITLE\"]])\n","\n","\n","def title_to_ids(t):\n","  res = [0 for _ in range(num_words_of_title)]\n","  mask = [0 for _ in range(num_words_of_title)]\n","  for i, w in enumerate(t.split()):\n","    if w in word_ids.keys():\n","      res[i] = word_ids[w]\n","      mask[i] = 1\n","  return res, mask\n","\n","\n","res, mask = title_to_ids(\"Europe reaches crunch point on banking union\")\n","print(res[:5])\n","print(mask[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-B67Lue2AVb6"},"source":["X_train = torch.tensor([title_to_ids(title) for title in df_train[\"TITLE\"]])\n","y_train = torch.tensor(df_train[\"CATEGORY\"].values.astype(\"int\"))\n","X_valid = torch.tensor([title_to_ids(title) for title in df_valid[\"TITLE\"]])\n","y_valid = torch.tensor(df_valid[\"CATEGORY\"].values.astype(\"int\"))\n","print(X_train[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5wE4AZhABbV"},"source":["batch_size = 64\n","num_workers = 2\n","dataset_train = [(X_i[0], X_i[1], y_i) for X_i, y_i in zip(X_train, y_train)]\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n","dataset_valid = [(X_i[0], X_i[1], y_i) for X_i, y_i in zip(X_valid, y_valid)]\n","dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True, num_workers=num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aw9Lz8jaKjdI"},"source":["hidden_size = 768\n","\n","class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n","    self.linear = nn.Linear(hidden_size, 4)\n","    self.softmax = nn.Softmax(dim=1)\n","    \n","  def forward(self, x, mask):\n","    y = self.bert(x, attention_mask=mask)\n","    y = self.linear(y.pooler_output)\n","    y = self.softmax(y)\n","    return y\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","print(f\"device: {device}\")\n","model = Model().to(device)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PCgc_uWw-jZ"},"source":["learning_rate = 0.05\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = SGD(model.linear.parameters(), lr=learning_rate)\n","scheduler = ExponentialLR(optimizer, gamma=0.95)\n","\n","loss_train = []\n","correct_train = []\n","loss_valid = []\n","correct_valid = []\n","\n","for epoch in range(5):\n","  print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n","  size = len(dataloader_train.dataset)\n","  for batch, (X, mask, y) in enumerate(dataloader_train):\n","    X, mask, y = X.to(device), mask.to(device), y.to(device)\n","    loss = loss_fn(model(X, mask), y)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    if batch % 10 == 0:\n","      loss, current = loss.item(), batch * len(X)\n","      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","  scheduler.step()\n","\n","  size = len(dataloader_train.dataset)\n","  loss, correct = 0, 0\n","  with torch.no_grad():\n","    for X, mask, y in dataloader_train:\n","      X, mask, y = X.to(device), mask.to(device), y.to(device)\n","      pred = model(X, mask)\n","      loss += loss_fn(pred, y).item()\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","  loss /= size\n","  correct /= size\n","  loss_train.append(loss)\n","  correct_train.append(correct)\n","\n","  size = len(dataloader_valid.dataset)\n","  loss, correct = 0, 0\n","  with torch.no_grad():\n","    for X, mask, y in dataloader_valid:\n","      X, mask, y = X.to(device), mask.to(device), y.to(device)\n","      pred = model(X, mask)\n","      loss += loss_fn(pred, y).item()\n","      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","  loss /= size\n","  correct /= size\n","  loss_valid.append(loss)\n","  correct_valid.append(correct)\n","  print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {loss:>8f} \\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEGqRHQ2SJmV"},"source":["plt.plot(loss_train, label=\"train\")\n","plt.plot(loss_valid, label=\"valid\")\n","plt.title(\"Loss\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQ4ClDaiELNE"},"source":["plt.plot(correct_train, label=\"train\")\n","plt.plot(correct_valid, label=\"valid\")\n","plt.title(\"Accuracy\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wy7liF8kE4Gi"},"source":[""],"execution_count":null,"outputs":[]}]}